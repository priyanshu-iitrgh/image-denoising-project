{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7pxipfoNmGNuoWUeXFVYe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/priyanshu-iitrgh/image-denoising-project/blob/main/image_denoising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "z4vyQqz7XSen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "MAX_TRAIN_IMAGES = 400"
      ],
      "metadata": {
        "id": "h6_IWiFSwcsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_low_light_images  = sorted(glob(\"./Train/low/*\"))[:MAX_TRAIN_IMAGES]\n",
        "val_low_light_images  = sorted(glob(\"./Train/low/*\"))[MAX_TRAIN_IMAGES:]\n",
        "\n",
        "# Verify that the lists are populated correctly\n",
        "print(\"Number of training images:\", len(train_low_light_images))\n",
        "print(\"Number of validation images:\", len(val_low_light_images))\n",
        "\n",
        "# Print a few file paths to check if they are valid\n",
        "print(\"Sample training images:\")\n",
        "for i in range(5):\n",
        "    if i < len(train_low_light_images):\n",
        "        print(train_low_light_images[i])\n",
        "\n",
        "print(\"\\nSample validation images:\")\n",
        "for i in range(5):\n",
        "    if i < len(val_low_light_images):\n",
        "        print(val_low_light_images[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YKGzG7XwqfG",
        "outputId": "d859115f-4776-4e86-fff2-f7ebbcdc982a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training images: 400\n",
            "Number of validation images: 84\n",
            "Sample training images:\n",
            "./Train/low/100.png\n",
            "./Train/low/101.png\n",
            "./Train/low/102.png\n",
            "./Train/low/103.png\n",
            "./Train/low/104.png\n",
            "\n",
            "Sample validation images:\n",
            "./Train/low/720.png\n",
            "./Train/low/721.png\n",
            "./Train/low/722.png\n",
            "./Train/low/723.png\n",
            "./Train/low/724.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(image_path):\n",
        "        image = tf.io.read_file(image_path)\n",
        "        image = tf.image.decode_png(image, channels = 3)\n",
        "        image = tf.image.resize(images=image, size=[IMAGE_SIZE, IMAGE_SIZE])\n",
        "        image = image/255.0\n",
        "        return image\n",
        "\n",
        "def data_generator(low_light_images):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
        "        dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.batch(BATCH_SIZE, drop_remainder = True )\n",
        "        return dataset"
      ],
      "metadata": {
        "id": "GG72CvzKw7lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = data_generator(train_low_light_images)\n",
        "val_dataset = data_generator(val_low_light_images)\n",
        "\n",
        "print(\"Train Dataset: \", train_dataset)\n",
        "print(\"Validation Dataset: \", val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PtaPvL-wqiO",
        "outputId": "584b665c-8866-4d0d-d336-10cc97824a16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset:  <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n",
            "Validation Dataset:  <_BatchDataset element_spec=TensorSpec(shape=(16, 256, 256, 3), dtype=tf.float32, name=None)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dce_net():\n",
        "    input_img = keras.Input(shape = [None , None, 3])\n",
        "    convl = layers.Conv2D(\n",
        "        32, (3,3), strides = (1,1),  activation = 'relu', padding = 'same'\n",
        "        )(input_img)\n",
        "    conv2 = layers.Conv2D(\n",
        "        32, (3,3), strides = (1,1),  activation = 'relu', padding = 'same'\n",
        "        )(convl)\n",
        "    conv3 = layers.Conv2D(\n",
        "        32, (3,3), strides = (1,1),  activation = 'relu', padding = 'same'\n",
        "        )(conv2)\n",
        "    conv4 = layers.Conv2D(\n",
        "        32, (3,3), strides = (1,1),  activation = 'relu', padding = 'same'\n",
        "        )(conv3)\n",
        "    int_con1 = layers.Concatenate(axis= -1)([conv4, conv3])\n",
        "    conv5 = layers.Conv2D(\n",
        "        32, (3,3), strides = (1,1),  activation = 'relu', padding = 'same'\n",
        "        )(int_con1)\n",
        "    int_con2 = layers.Concatenate(axis= -1)([conv5, conv2])\n",
        "    conv6 = layers.Conv2D(\n",
        "        32, (3,3), strides = (1,1),  activation = 'relu', padding = 'same'\n",
        "        )(int_con2)\n",
        "    int_con3 = layers.Concatenate(axis= -1)([conv6, convl])\n",
        "    x_r = layers.Conv2D(\n",
        "        24, (3,3), strides = (1,1),  activation = 'tanh', padding = 'same'\n",
        "        )(int_con3)\n",
        "    return keras.Model(inputs = input_img, outputs = x_r)\n"
      ],
      "metadata": {
        "id": "eajCHRkzwqlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def color_constancy_loss(x):\n",
        "    mean_rgb = tf.reduce_mean(x, axis = (1,2), keepdims= True)\n",
        "    mr, mg, mb = mean_rgb[:,:,:,0], mean_rgb[:,:,:,1], mean_rgb[:,:,:,2]\n",
        "    d_rg = tf.square(mr - mg)\n",
        "    d_gb = tf.square(mg - mb)\n",
        "    d_rb = tf.square(mr - mb)\n",
        "    return tf.sqrt(tf.square(d_rg) + tf.square(d_gb) + tf.square(d_rb))"
      ],
      "metadata": {
        "id": "MDZskde91EAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exposure_loss(x, mean_val = 0.6):\n",
        "    x = tf.reduce_mean(x, axis = 3, keepdims = True)\n",
        "    means = tf.nn.avg_pool2d(x, ksize = 16, strides = 16, padding = 'VALID')\n",
        "    return tf.reduce_mean(tf.square(means - mean_val))\n"
      ],
      "metadata": {
        "id": "UmYr7Dfgwqxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def illumination_smoothness_loss(x):\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    h_x = tf.shape(x)[1]\n",
        "    w_x = tf.shape(x)[2]\n",
        "    count_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\n",
        "    count_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\n",
        "    h_tv = tf.reduce_sum(tf.square((x[:,1:,:,:] - x[:,:h_x-1,:,:])))\n",
        "    w_tv = tf.reduce_sum(tf.square((x[:,:,1:,:] - x[:,:,:w_x-1,:])))\n",
        "    batch_size = tf.cast(batch_size, dtype = tf.float32)\n",
        "    count_h = tf.cast(count_h, dtype = tf.float32)\n",
        "    count_w = tf.cast(count_w, dtype = tf.float32)\n",
        "    return 2 * (h_tv/count_h + w_tv/count_w)/batch_size"
      ],
      "metadata": {
        "id": "LcDW863rwq0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialConsistencyLoss(keras.losses.Loss):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(SpatialConsistencyLoss, self).__init__(reduction = \"none\")\n",
        "\n",
        "        self.left_kernel = tf.constant(\n",
        "            [[[[0,0,0]], [[-1,-1,0]], [[0,0,0]]]], dtype = tf.float32\n",
        "            )\n",
        "        self.right_kernel = tf.constant(\n",
        "            [[[[0,0,0]], [[0,1,-1]], [[0,0,0]]]], dtype = tf.float32\n",
        "            )\n",
        "        self.up_kernel = tf.constant(\n",
        "            [[[[0,-1,0]], [[0,1,0]], [[0,0,0]]]], dtype = tf.float32\n",
        "            )\n",
        "        self.down_kernel = tf.constant(\n",
        "            [[[[0,0,0]], [[0,1,0]], [[0,-1,0]]]], dtype = tf.float32\n",
        "            )\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "\n",
        "        original_mean = tf.reduce_mean(y_true, 3, keepdims = True)\n",
        "        enchanced_mean = tf.reduce_mean(y_pred, 3, keepdims = True)\n",
        "        original_pool = tf.nn.avg_pool2d(\n",
        "            original_mean, ksize = 4, strides = 4, padding = 'VALID'\n",
        "            )\n",
        "        enchanced_pool = tf.nn.avg_pool2d(\n",
        "            enchanced_mean, ksize = 4, strides = 4, padding = 'VALID'\n",
        "            )\n",
        "\n",
        "        d_original_left = tf.nn.conv2d(\n",
        "            original_pool, self.left_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "        d_original_right = tf.nn.conv2d(\n",
        "            original_pool, self.right_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "        d_original_up = tf.nn.conv2d(\n",
        "            original_pool, self.up_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "        d_original_down = tf.nn.conv2d(\n",
        "            original_pool, self.down_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "\n",
        "        d_enchanced_left = tf.nn.conv2d(\n",
        "            enchanced_pool, self.left_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "        d_enchanced_right = tf.nn.conv2d(\n",
        "            enchanced_pool, self.right_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "        d_enchanced_up = tf.nn.conv2d(\n",
        "            enchanced_pool, self.up_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "        d_enchanced_down = tf.nn.conv2d(\n",
        "            enchanced_pool, self.down_kernel, strides = [1,1,1,1], padding = 'SAME'\n",
        "            )\n",
        "\n",
        "        d_left = tf.square(d_original_left - d_enchanced_left)\n",
        "        d_right = tf.square(d_original_right - d_enchanced_right)\n",
        "        d_up = tf.square(d_original_up - d_enchanced_up)\n",
        "        d_down = tf.square(d_original_down - d_enchanced_down)\n",
        "\n",
        "        return d_left + d_right + d_up + d_down\n"
      ],
      "metadata": {
        "id": "y0_hibG7wq32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ZeroDCE(keras.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ZeroDCE, self).__init__(**kwargs)\n",
        "        self.dce_model = build_dce_net()\n",
        "\n",
        "    def compile(self, learning_rate, **kwargs):\n",
        "        super(ZeroDCE, self).compile(**kwargs)\n",
        "        self.optimizer = keras.optimizers.Adam(learning_rate = learning_rate)\n",
        "        self.spatial_consistency_loss = SpatialConsistencyLoss(reduction = \"none\")\n",
        "\n",
        "    def get_enchanced_image(self, data, output):\n",
        "      r1 = output[: , : , : , :3]\n",
        "      r2 = output[: , : , : , 3:6]\n",
        "      r3 = output[: , : , : , 6:9]\n",
        "      r4 = output[: , : , : , 9:12]\n",
        "      r5 = output[: , : , : , 12:15]\n",
        "      r6 = output[: , : , : , 15:18]\n",
        "      r7 = output[: , : , : , 18:21]\n",
        "      r8 = output[: , : , : , 21:24]\n",
        "      x = data + r1 * (tf.square(data) - data)\n",
        "      x = x + r2 * (tf.square(x) - x)\n",
        "      x = x + r3 * (tf.square(x) - x)\n",
        "      enhanced_image = x + r4 * (tf.square(x) - x)\n",
        "      x = enhanced_image + r5 * (tf.square(enhanced_image) - enhanced_image)\n",
        "      x = x + r6 * (tf.square(x) - x)\n",
        "      x = x + r7 * (tf.square(x) - x)\n",
        "      enhanced_image = x + r8 * (tf.square(x) - x)\n",
        "      return enhanced_image\n",
        "\n",
        "    def call(self, data ):\n",
        "      dce_net_output = self.dce_model(data)\n",
        "      return self.get_enchanced_image(data, dce_net_output)\n",
        "\n",
        "    def compute_losses(self, data, output):\n",
        "      enchanced_image = self.get_enchanced_image(data, output)\n",
        "      loss_illumination = 200 * illumination_smoothness_loss(enchanced_image)\n",
        "      loss_spatial_constancy = tf.reduce_mean(\n",
        "          self.spatial_consistency_loss(data, enchanced_image)\n",
        "          )\n",
        "      loss_color_constancy = 5 * tf.reduce_mean(\n",
        "          color_constancy_loss(enchanced_image)\n",
        "          )\n",
        "      loss_exposure = 10 * tf.reduce_mean(\n",
        "          exposure_loss(enchanced_image)\n",
        "          )\n",
        "      total_loss = (\n",
        "          loss_illumination\n",
        "          + loss_spatial_constancy\n",
        "          + loss_color_constancy\n",
        "          + loss_exposure\n",
        "      )\n",
        "      return {\n",
        "          \"total loss\": total_loss,\n",
        "          \"illumination_smoothness_loss\": loss_illumination,\n",
        "          \"spatial_constancy loss\": loss_spatial_constancy,\n",
        "          \"color_constancy loss\": loss_color_constancy,\n",
        "          \"exposure loss\": loss_exposure,\n",
        "      }\n",
        "\n",
        "    def train_step(self, data):\n",
        "      with tf.GradientTape() as tape:\n",
        "        output = self.dce_model(data)\n",
        "        losses = self.compute_losses(data, output)\n",
        "      gradients = tape.gradient(\n",
        "          losses[\"total loss\"], self.dce_model.trainable_weights\n",
        "          )\n",
        "      self.optimizer.apply_gradients(zip(gradients, self.dce_model.trainable_weights))\n",
        "      return losses\n",
        "\n",
        "    def test_step(self, data):\n",
        "      output = self.dce_model(data)\n",
        "      losses = self.compute_losses(data, output)\n",
        "      return losses\n",
        "\n",
        "    def save_weights(self, filepath, overwrite = True, save_format = None, options = None):\n",
        "      self.dce_model.save_weights(\n",
        "          filepath, overwrite = overwrite, save_format = save_format, options = options\n",
        "          )\n",
        "\n",
        "    def load_weights(self, filepath, by_name = False, skip_mismatch = False, options = None):\n",
        "      self.dce_model.load_weights(\n",
        "          filepath, by_name = by_name, skip_mismatch = skip_mismatch, options = options\n",
        "      )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jGHh09b7wq6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zero_dce_model = ZeroDCE()\n",
        "zero_dce_model.compile(learning_rate = 1e-4)\n",
        "history = zero_dce_model.fit(train_dataset, epochs = 100, validation_data = val_dataset)\n",
        "\n",
        "def plot_result(item):\n",
        "    plt.plot(history.history[item], label = item)\n",
        "    plt.plot(history.history[\"val_\" + item], label = \"val_\" + item)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(item)\n",
        "    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize = 14)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_result(\"total loss\")\n",
        "plot_result(\"illumination_smoothness_loss\")\n",
        "plot_result(\"spatial_constancy loss\")\n",
        "plot_result(\"color_constancy loss\")\n",
        "plot_result(\"exposure loss\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCb-TZe4-X1N",
        "outputId": "f8f7b611-1f2e-43a7-8f49-07c8759489d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "25/25 [==============================] - 512s 20s/step - total loss: 47.5292 - illumination_smoothness_loss: 44.5001 - spatial_constancy loss: 7.9431e-04 - color_constancy loss: 0.0026 - exposure loss: 3.0257 - val_total loss: 39.9811 - val_illumination_smoothness_loss: 36.8290 - val_spatial_constancy loss: 0.0017 - val_color_constancy loss: 4.6860e-04 - val_exposure loss: 3.1499\n",
            "Epoch 2/100\n",
            "25/25 [==============================] - 502s 20s/step - total loss: 16.5890 - illumination_smoothness_loss: 13.2603 - spatial_constancy loss: 0.0066 - color_constancy loss: 0.0010 - exposure loss: 3.3212 - val_total loss: 36.0967 - val_illumination_smoothness_loss: 32.5755 - val_spatial_constancy loss: 0.0091 - val_color_constancy loss: 1.4584e-04 - val_exposure loss: 3.5119\n",
            "Epoch 3/100\n",
            "25/25 [==============================] - 498s 20s/step - total loss: 5.6252 - illumination_smoothness_loss: 2.0473 - spatial_constancy loss: 0.0107 - color_constancy loss: 2.1972e-05 - exposure loss: 3.5672 - val_total loss: 36.1428 - val_illumination_smoothness_loss: 32.5483 - val_spatial_constancy loss: 0.0102 - val_color_constancy loss: 8.5447e-06 - val_exposure loss: 3.5843\n",
            "Epoch 4/100\n",
            "25/25 [==============================] - 504s 20s/step - total loss: 5.1713 - illumination_smoothness_loss: 1.5675 - spatial_constancy loss: 0.0110 - color_constancy loss: 1.3443e-06 - exposure loss: 3.5927 - val_total loss: 36.2704 - val_illumination_smoothness_loss: 32.6700 - val_spatial_constancy loss: 0.0102 - val_color_constancy loss: 4.8257e-06 - val_exposure loss: 3.5902\n",
            "Epoch 5/100\n",
            "25/25 [==============================] - 502s 20s/step - total loss: 5.1180 - illumination_smoothness_loss: 1.5107 - spatial_constancy loss: 0.0111 - color_constancy loss: 3.9905e-07 - exposure loss: 3.5963 - val_total loss: 36.3310 - val_illumination_smoothness_loss: 32.7284 - val_spatial_constancy loss: 0.0102 - val_color_constancy loss: 3.9516e-06 - val_exposure loss: 3.5924\n",
            "Epoch 6/100\n",
            "25/25 [==============================] - ETA: 0s - total loss: 4.9522 - illumination_smoothness_loss: 1.3436 - spatial_constancy loss: 0.0108 - color_constancy loss: 1.4232e-07 - exposure loss: 3.5978 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_result(images, titles, figure_size=(12,12)):\n",
        "  fig = plt.figure(figsize = figure_size)\n",
        "  for i in range(len(images)):\n",
        "    fig.add_subplot(1, len(images), i+1).set_title(titles[i])\n",
        "    _ = plt.imshow(images[i])\n",
        "    plt.axis(\"off\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def infer(original_image):\n",
        "  image = keras.preprocessing.image.img_to_array(original_image)\n",
        "  image = image.astype(\"float32\")/255.0\n",
        "  image = np.expand_dims(image, axis = 0)\n",
        "  output_image = zero_dce_image(image)\n",
        "  output_image - tf.cast((output_image[0,:,:,:] * 255), dtype = tf.uint8)\n",
        "  output_image = Image.fromarray(output_image.numpy())\n",
        "  return output_image"
      ],
      "metadata": {
        "id": "26bdQ9Wg-XuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for val_image_file in test_low_light_images:\n",
        "  original_image = Image.open(val_image_file)\n",
        "  enhanced_image = infer(original_image)\n",
        "  plot_result(\n",
        "      [original_image, ImageOps.autocontrast(original_image),enhanced_image],\n",
        "      [\"Original\", \"PIL Autocontrast\" , \"Enhanced\"],\n",
        "      (20,12)\n",
        "  )"
      ],
      "metadata": {
        "id": "TSXszSc1-XrF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}